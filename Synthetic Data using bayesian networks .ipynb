{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2cb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from itertools import combinations, product, islice, chain\n",
    "from math import log, ceil\n",
    "from multiprocessing.pool import Pool\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from string import ascii_lowercase\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from pandas import merge\n",
    "from scipy.optimize import fsolve\n",
    "from sklearn.metrics import mutual_info_score, normalized_mutual_info_score\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from bisect import bisect_right\n",
    "from random import uniform\n",
    "from typing import List, Union\n",
    "from numpy.random import choice\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "from numpy import array_equal\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343772d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataType(Enum):\n",
    "    INTEGER = 'Integer'\n",
    "    FLOAT = 'Float'\n",
    "    STRING = 'String'\n",
    "    DATETIME = 'DateTime'\n",
    "    SOCIAL_SECURITY_NUMBER = 'SocialSecurityNumber'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_attributes_mutual_information(dataset):\n",
    "    \"\"\"Compute normalized mutual information for all pairwise attributes. Return a DataFrame.\"\"\"\n",
    "    sorted_columns = sorted(dataset.columns)\n",
    "    mi_df = DataFrame(columns=sorted_columns, index=sorted_columns, dtype=float)\n",
    "    for row in mi_df.columns:\n",
    "        for col in mi_df.columns:\n",
    "            mi_df.loc[row, col] = normalized_mutual_info_score(dataset[row].astype(str),\n",
    "                                                               dataset[col].astype(str),\n",
    "                                                               average_method='arithmetic')\n",
    "    return mi_df\n",
    "def read_json_file(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def infer_numerical_attributes_in_dataframe(dataframe):\n",
    "    describe = dataframe.describe()\n",
    "    # DataFrame.describe() usually returns 8 rows.\n",
    "    if describe.shape[0] == 8:\n",
    "        return set(describe.columns)\n",
    "    # DataFrame.describe() returns less than 8 rows when there is no numerical attribute.\n",
    "    else:\n",
    "        return set()\n",
    "def generate_random_string(length):\n",
    "    return ''.join(np.random.choice(list(ascii_lowercase), size=length))\n",
    "def set_random_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "def mutual_information(labels_x: Series, labels_y: DataFrame):\n",
    "# Mutual information of distributions in format of Series or DataFrame using inbuilt mutual_info_score lib fromsklearn.metrics\n",
    "#   Parameters\n",
    "# ----------\n",
    "# labels_x : Series\n",
    "#labels_y : DataFrame\n",
    "    if labels_y.shape[1] == 1:\n",
    "        labels_y = labels_y.iloc[:, 0]\n",
    "    else:\n",
    "        labels_y = labels_y.apply(lambda x: ' '.join(x.values), axis=1)\n",
    "\n",
    "    return mutual_info_score(labels_x, labels_y)\n",
    "def normalize_given_distribution(frequencies):\n",
    "    distribution = np.array(frequencies, dtype=float)\n",
    "    distribution = distribution.clip(0)  # replace negative values with 0\n",
    "    summation = distribution.sum()\n",
    "    if summation > 0:\n",
    "        if np.isinf(summation):\n",
    "            return normalize_given_distribution(np.isinf(distribution))\n",
    "        else:\n",
    "            return distribution / summation\n",
    "    else:\n",
    "        return np.full_like(distribution, 1 / distribution.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15dcf0e",
   "metadata": {},
   "source": [
    "The Format in which we save the info of the attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractAttribute(object):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, name: str, is_candidate_key, is_categorical, histogram_size: Union[int, str], data: Series):\n",
    "        self.name = name\n",
    "        self.is_candidate_key = is_candidate_key\n",
    "        self.is_categorical = is_categorical\n",
    "        self.histogram_size: Union[int, str] = histogram_size\n",
    "        self.data: Series = data\n",
    "        self.data_dropna: Series = self.data.dropna()\n",
    "        self.missing_rate: float = (self.data.size - self.data_dropna.size) / (self.data.size or 1)\n",
    "\n",
    "        self.is_numerical: bool = None\n",
    "        self.data_type: DataType = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "        self.distribution_bins: np.ndarray = None\n",
    "        self.distribution_probabilities: np.ndarray = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def infer_domain(self, categorical_domain: List = None, numerical_range: List = None):\n",
    " #       \"\"\"Infer categorical_domain, including min, max, and 1-D distribution.\n",
    "\n",
    "#        \"\"\"\n",
    "        if categorical_domain:\n",
    "            self.min = min(categorical_domain)\n",
    "            self.max = max(categorical_domain)\n",
    "            self.distribution_bins = np.array(categorical_domain)\n",
    "        elif numerical_range:\n",
    "            self.min, self.max = numerical_range\n",
    "            self.distribution_bins = np.array([self.min, self.max])\n",
    "        else:\n",
    "            self.min = float(self.data_dropna.min())\n",
    "            self.max = float(self.data_dropna.max())\n",
    "            if self.is_categorical:\n",
    "                self.distribution_bins = self.data_dropna.unique()\n",
    "            else:\n",
    "                self.distribution_bins = np.array([self.min, self.max])\n",
    "\n",
    "        self.distribution_probabilities = np.full_like(self.distribution_bins, 1 / self.distribution_bins.size)\n",
    "\n",
    "    @abstractmethod\n",
    "    def infer_distribution(self):\n",
    "        if self.is_categorical:\n",
    "            distribution = self.data_dropna.value_counts()\n",
    "            for value in set(self.distribution_bins) - set(distribution.index):\n",
    "                distribution[value] = 0\n",
    "            distribution.sort_index(inplace=True)\n",
    "            self.distribution_probabilities = utils.normalize_given_distribution(distribution)\n",
    "            self.distribution_bins = np.array(distribution.index)\n",
    "        else:\n",
    "            distribution = np.histogram(self.data_dropna, bins=self.histogram_size, range=(self.min, self.max))\n",
    "            self.distribution_bins = distribution[1][:-1]  # Remove the last bin edge\n",
    "            self.distribution_probabilities = utils.normalize_given_distribution(distribution[0])\n",
    "\n",
    "    def inject_laplace_noise(self, epsilon, num_valid_attributes):\n",
    "        if epsilon > 0:\n",
    "            sensitivity = 2 / self.data.size\n",
    "            privacy_budget = epsilon / num_valid_attributes\n",
    "            noise_scale = sensitivity / privacy_budget\n",
    "            laplace_noises = np.random.laplace(0, scale=noise_scale, size=len(self.distribution_probabilities))\n",
    "            noisy_distribution = self.distribution_probabilities + laplace_noises\n",
    "            self.distribution_probabilities = utils.normalize_given_distribution(noisy_distribution)\n",
    "\n",
    "    def encode_values_into_bin_idx(self):\n",
    "#        \"\"\"Encode values into bin indices for Bayesian Network construction.\n",
    "\n",
    " #       \"\"\"\n",
    "        if self.is_categorical:\n",
    "            value_to_bin_idx = {value: idx for idx, value in enumerate(self.distribution_bins)}\n",
    "            encoded = self.data.map(lambda x: value_to_bin_idx[x], na_action='ignore')\n",
    "        else:\n",
    "            encoded = self.data.map(lambda x: bisect_right(self.distribution_bins, x) - 1, na_action='ignore')\n",
    "\n",
    "        encoded.fillna(len(self.distribution_bins), inplace=True)\n",
    "        return encoded.astype(int, copy=False)\n",
    "\n",
    "    def to_json(self):\n",
    "     #   \"\"\"Encode attribution information in JSON format / Python dictionary.\n",
    "\n",
    "#        \"\"\"\n",
    "        return {\"name\": self.name,\n",
    "                \"data_type\": self.data_type.value,\n",
    "                \"is_categorical\": self.is_categorical,\n",
    "                \"is_candidate_key\": self.is_candidate_key,\n",
    "                \"min\": self.min,\n",
    "                \"max\": self.max,\n",
    "                \"missing_rate\": self.missing_rate,\n",
    "                \"distribution_bins\": self.distribution_bins.tolist(),\n",
    "                \"distribution_probabilities\": self.distribution_probabilities.tolist()}\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_values_as_candidate_key(self, n):\n",
    "        \n",
    "        return np.arange(n)\n",
    "\n",
    "    def sample_binning_indices_in_independent_attribute_mode(self, n):\n",
    "    #Sample an array of binning indices.\n",
    "\n",
    "        \n",
    "        return Series(choice(len(self.distribution_probabilities), size=n, p=self.distribution_probabilities))\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_values_from_binning_indices(self, binning_indices):\n",
    "        #Convert binning indices into values in domain. Used by both independent and correlated attribute mode.\n",
    "\n",
    "        \n",
    "        return binning_indices.apply(lambda x: self.uniform_sampling_within_a_bin(x))\n",
    "\n",
    "    def uniform_sampling_within_a_bin(self, bin_idx: int):\n",
    "        num_bins = len(self.distribution_bins)\n",
    "        if bin_idx == num_bins:\n",
    "            return np.nan\n",
    "        elif self.is_categorical:\n",
    "            return self.distribution_bins[bin_idx]\n",
    "        elif bin_idx < num_bins - 1:\n",
    "            return uniform(self.distribution_bins[bin_idx], self.distribution_bins[bin_idx + 1])\n",
    "        else:\n",
    "            # sample from the last interval where the right edge is missing in self.distribution_bins\n",
    "            neg_2, neg_1 = self.distribution_bins[-2:]\n",
    "            return uniform(neg_1, self.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7fd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatAttribute(AbstractAttribute):\n",
    "    def __init__(self, name: str, is_candidate_key, is_categorical, histogram_size: Union[int, str], data: Series):\n",
    "        super().__init__(name, is_candidate_key, is_categorical, histogram_size, data)\n",
    "        self.is_numerical = True\n",
    "        self.data_type = DataType.FLOAT\n",
    "\n",
    "    def infer_domain(self, categorical_domain=None, numerical_range=None):\n",
    "        super().infer_domain(categorical_domain, numerical_range)\n",
    "\n",
    "    def infer_distribution(self):\n",
    "        super().infer_distribution()\n",
    "\n",
    "    def generate_values_as_candidate_key(self, n):\n",
    "        return arange(self.min, self.max, (self.max - self.min) / n)\n",
    "\n",
    "    def sample_values_from_binning_indices(self, binning_indices):\n",
    "        return super().sample_values_from_binning_indices(binning_indices)\n",
    "class IntegerAttribute(AbstractAttribute):\n",
    "    def __init__(self, name: str, is_candidate_key, is_categorical, histogram_size: Union[int, str], data: Series):\n",
    "        super().__init__(name, is_candidate_key, is_categorical, histogram_size, data)\n",
    "        self.is_numerical = True\n",
    "        self.data_type = DataType.INTEGER\n",
    "\n",
    "    def infer_domain(self, categorical_domain=None, numerical_range=None):\n",
    "        super().infer_domain(categorical_domain, numerical_range)\n",
    "        self.min = int(self.min)\n",
    "        self.max = int(self.max)\n",
    "\n",
    "    def infer_distribution(self):\n",
    "        super().infer_distribution()\n",
    "\n",
    "    def generate_values_as_candidate_key(self, n):\n",
    "        return super().generate_values_as_candidate_key(n)\n",
    "\n",
    "    def sample_values_from_binning_indices(self, binning_indices):\n",
    "        column = super().sample_values_from_binning_indices(binning_indices)\n",
    "        column = column.round()\n",
    "        column[~column.isnull()] = column[~column.isnull()].astype(int)\n",
    "        return column\n",
    "def is_datetime(value: str):\n",
    " #   \"\"\"Find whether a value is a datetime. Here weekdays and months are categorical values instead of datetime.\"\"\"\n",
    "    weekdays = {'mon', 'monday', 'tue', 'tuesday', 'wed', 'wednesday', 'thu', 'thursday', 'fri', 'friday',\n",
    "                'sat', 'saturday', 'sun', 'sunday'}\n",
    "    months = {'jan', 'january', 'feb', 'february', 'mar', 'march', 'apr', 'april', 'may', 'may', 'jun', 'june',\n",
    "              'jul', 'july', 'aug', 'august', 'sep', 'sept', 'september', 'oct', 'october', 'nov', 'november',\n",
    "              'dec', 'december'}\n",
    "\n",
    "    value_lower = value.lower()\n",
    "    if (value_lower in weekdays) or (value_lower in months):\n",
    "        return False\n",
    "    try:\n",
    "        parse(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# TODO detect datetime formats\n",
    "class DateTimeAttribute(AbstractAttribute):\n",
    "    def __init__(self, name: str, is_candidate_key, is_categorical, histogram_size: Union[int, str], data: Series):\n",
    "        super().__init__(name, is_candidate_key, is_categorical, histogram_size, data)\n",
    "        self.is_numerical = True\n",
    "        self.data_type = DataType.DATETIME\n",
    "        epoch_datetime = parse('1970-01-01')\n",
    "        self.timestamps = self.data_dropna.map(lambda x: int((parse(x) - epoch_datetime).total_seconds()))\n",
    "\n",
    "    def infer_domain(self, categorical_domain=None, numerical_range=None):\n",
    "        if numerical_range:\n",
    "            self.min, self.max = numerical_range\n",
    "            self.distribution_bins = np.array([self.min, self.max])\n",
    "        else:\n",
    "            self.min = float(self.timestamps.min())\n",
    "            self.max = float(self.timestamps.max())\n",
    "            if self.is_categorical:\n",
    "                self.distribution_bins = self.data_dropna.unique()\n",
    "            else:\n",
    "                self.distribution_bins = np.array([self.min, self.max])\n",
    "\n",
    "        self.distribution_probabilities = np.full_like(self.distribution_bins, 1 / self.distribution_bins.size)\n",
    "\n",
    "    def infer_distribution(self):\n",
    "        if self.is_categorical:\n",
    "            distribution = self.data_dropna.value_counts()\n",
    "            for value in set(self.distribution_bins) - set(distribution.index):\n",
    "                distribution[value] = 0\n",
    "            distribution.sort_index(inplace=True)\n",
    "            self.distribution_probabilities = normalize_given_distribution(distribution)\n",
    "            self.distribution_bins = np.array(distribution.index)\n",
    "        else:\n",
    "            distribution = np.histogram(self.timestamps, bins=self.histogram_size, range=(self.min, self.max))\n",
    "            self.distribution_probabilities = normalize_given_distribution(distribution[0])\n",
    "\n",
    "    def encode_values_into_bin_idx(self):\n",
    "#        \"\"\"Encode values into bin indices for Bayesian Network construction.\n",
    "\n",
    "#        \"\"\"\n",
    "        if self.is_categorical:\n",
    "            value_to_bin_idx = {value: idx for idx, value in enumerate(self.distribution_bins)}\n",
    "            encoded = self.data.map(lambda x: value_to_bin_idx[x], na_action='ignore')\n",
    "        else:\n",
    "            encoded = self.timestamps.map(lambda x: bisect_right(self.distribution_bins, x) - 1, na_action='ignore')\n",
    "            encoded = concat([encoded, self.data], axis=1).iloc[:, 0]\n",
    "\n",
    "        encoded.fillna(len(self.distribution_bins), inplace=True)\n",
    "        return encoded.astype(int, copy=False)\n",
    "\n",
    "    def generate_values_as_candidate_key(self, n):\n",
    "        return np.arange(self.min, self.max, (self.min - self.max) / n)\n",
    "\n",
    "    def sample_values_from_binning_indices(self, binning_indices):\n",
    "        column = super().sample_values_from_binning_indices(binning_indices)\n",
    "        if not self.is_categorical:\n",
    "            column[~column.isnull()] = column[~column.isnull()].astype(int)\n",
    "        return column\n",
    "#for string attribute\n",
    "class StringAttribute(AbstractAttribute):\n",
    " #   \"\"\"Variable min and max are the lengths of the shortest and longest strings.\n",
    "\n",
    "#    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, is_candidate_key, is_categorical, histogram_size: Union[int, str], data: Series):\n",
    "        super().__init__(name, is_candidate_key, is_categorical, histogram_size, data)\n",
    "        self.is_numerical = False\n",
    "        self.data_type = DataType.STRING\n",
    "        self.data_dropna = self.data_dropna.astype(str)\n",
    "        self.data_dropna_len = self.data_dropna.map(len)\n",
    "\n",
    "    def infer_domain(self, categorical_domain=None, numerical_range=None):\n",
    "        if categorical_domain:\n",
    "            lengths = [len(i) for i in categorical_domain]\n",
    "            self.min = min(lengths)\n",
    "            self.max = max(lengths)\n",
    "            self.distribution_bins = np.array(categorical_domain)\n",
    "        else:\n",
    "            self.min = int(self.data_dropna_len.min())\n",
    "            self.max = int(self.data_dropna_len.max())\n",
    "            if self.is_categorical:\n",
    "                self.distribution_bins = self.data_dropna.unique()\n",
    "            else:\n",
    "                self.distribution_bins = np.array([self.min, self.max])\n",
    "\n",
    "        self.distribution_probabilities = np.full_like(self.distribution_bins, 1 / self.distribution_bins.size)\n",
    "\n",
    "    def infer_distribution(self):\n",
    "        if self.is_categorical:\n",
    "            distribution = self.data_dropna.value_counts()\n",
    "            for value in set(self.distribution_bins) - set(distribution.index):\n",
    "                distribution[value] = 0\n",
    "            distribution.sort_index(inplace=True)\n",
    "            self.distribution_probabilities = utils.normalize_given_distribution(distribution)\n",
    "            self.distribution_bins = np.array(distribution.index)\n",
    "        else:\n",
    "            distribution = np.histogram(self.data_dropna_len, bins=self.histogram_size)\n",
    "            self.distribution_bins = distribution[1][:-1]\n",
    "            self.distribution_probabilities = utils.normalize_given_distribution(distribution[0])\n",
    "\n",
    "    def generate_values_as_candidate_key(self, n):\n",
    "        length = np.random.randint(self.min, self.max + 1)\n",
    "        vectorized = np.vectorize(lambda x: '{}{}'.format(utils.generate_random_string(length), x))\n",
    "        return vectorized(np.arange(n))\n",
    "\n",
    "    def sample_values_from_binning_indices(self, binning_indices):\n",
    "        column = super().sample_values_from_binning_indices(binning_indices)\n",
    "        if not self.is_categorical:\n",
    "            column[~column.isnull()] = column[~column.isnull()].apply(lambda x: utils.generate_random_string(int(x)))\n",
    "\n",
    "        return column\n",
    "def pre_process(column: Series):\n",
    "    if column.size == 0:\n",
    "        return column\n",
    "    elif type(column.iloc[0]) is int:\n",
    "        return column\n",
    "    elif type(column.iloc[0]) is str:\n",
    "        return column.map(lambda x: int(x.replace('-', '')))\n",
    "    else:\n",
    "        raise Exception('Invalid SocialSecurityNumber.')\n",
    "\n",
    "#if attribute is socialsecuritynumber\n",
    "def is_ssn(value):\n",
    "#   Test whether a number is between 0 and 1e9.\n",
    "\n",
    "#    Note this function does not take into consideration some special numbers that are never allocated.\n",
    " #   https://en.wikipedia.org/wiki/Social_Security_number\n",
    " ##   \"\"\"\n",
    "    if type(value) is int:\n",
    "        return 0 < value < 1e9\n",
    "    elif type(value) is str:\n",
    "        value = value.replace('-', '')\n",
    "        if value.isdigit():\n",
    "            return 0 < int(value) < 1e9\n",
    "    return False\n",
    "\n",
    "\n",
    "class SocialSecurityNumberAttribute(AbstractAttribute):\n",
    "#    \"\"\"SocialSecurityNumber of format AAA-GG-SSSS.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, is_candidate_key, is_categorical, histogram_size: Union[int, str], data: Series):\n",
    "        super().__init__(name, is_candidate_key, is_categorical, histogram_size, pre_process(data))\n",
    "        self.is_numerical = True\n",
    "        self.data_type = DataType.SOCIAL_SECURITY_NUMBER\n",
    "\n",
    "    def infer_domain(self, categorical_domain=None, numerical_range=None):\n",
    "        super().infer_domain(categorical_domain, numerical_range)\n",
    "        self.min = int(self.min)\n",
    "        self.max = int(self.max)\n",
    "\n",
    "    def infer_distribution(self):\n",
    "        super().infer_distribution()\n",
    "\n",
    "    def generate_values_as_candidate_key(self, n):\n",
    "        if n < 1e9:\n",
    "            values = np.linspace(0, 1e9 - 1, num=n, dtype=int)\n",
    "            values = np.random.permutation(values)\n",
    "            values = [str(i).zfill(9) for i in values]\n",
    "            return ['{}-{}-{}'.format(i[:3], i[3:5], i[5:]) for i in values]\n",
    "        else:\n",
    "            raise Exception('The candidate key \"{}\" cannot generate more than 1e9 distinct values.', self.name)\n",
    "\n",
    "    def sample_values_from_binning_indices(self, binning_indices):\n",
    "        return super().sample_binning_indices_in_independent_attribute_mode(binning_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fe85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will change the data intojson form before using for generation\n",
    "def parse_json(attribute_in_json):\n",
    "    name = attribute_in_json['name']\n",
    "    data_type = DataType(attribute_in_json['data_type'])\n",
    "    is_candidate_key = attribute_in_json['is_candidate_key']\n",
    "    is_categorical = attribute_in_json['is_categorical']\n",
    "    histogram_size = len(attribute_in_json['distribution_bins'])\n",
    "    if data_type is DataType.INTEGER:\n",
    "        attribute = IntegerAttribute(name, is_candidate_key, is_categorical, histogram_size, Series(dtype=int))\n",
    "    elif data_type is DataType.FLOAT:\n",
    "        attribute = FloatAttribute(name, is_candidate_key, is_categorical, histogram_size, Series(dtype=float))\n",
    "    elif data_type is DataType.DATETIME:\n",
    "        attribute = DateTimeAttribute(name, is_candidate_key, is_categorical, histogram_size, Series(dtype='datetime64[ns]'))\n",
    "    elif data_type is DataType.STRING:\n",
    "        attribute = StringAttribute(name, is_candidate_key, is_categorical, histogram_size, Series(dtype=str))\n",
    "    elif data_type is data_type.SOCIAL_SECURITY_NUMBER:\n",
    "        attribute = SocialSecurityNumberAttribute(name, is_candidate_key, is_categorical, histogram_size, Series(dtype=int))\n",
    "    else:\n",
    "        raise Exception('Data type {} is unknown.'.format(data_type.value))\n",
    "\n",
    "    attribute.missing_rate = attribute_in_json['missing_rate']\n",
    "    attribute.min = attribute_in_json['min']\n",
    "    attribute.max = attribute_in_json['max']\n",
    "    attribute.distribution_bins = attribute_in_json['distribution_bins']\n",
    "    attribute.distribution_probabilities = attribute_in_json['distribution_probabilities']\n",
    "\n",
    "    return attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for creating a approximate Bayesian network with noisy conditional distributions(data must be preprocessed(look into readme file))\n",
    "\n",
    "\n",
    "def calculate_sensitivity(num_tuples, child, parents, attr_to_is_binary):\n",
    "#Sensitivity function for Bayesian network construction.\n",
    "# num_tuples : int\n",
    "\n",
    "    if attr_to_is_binary[child] or (len(parents) == 1 and attr_to_is_binary[parents[0]]):\n",
    "        a = log(num_tuples) / num_tuples\n",
    "        b = (num_tuples - 1) / num_tuples\n",
    "        b_inv = num_tuples / (num_tuples - 1)\n",
    "        return a + b * log(b_inv)\n",
    "    else:\n",
    "        a = (2 / num_tuples) * log((num_tuples + 1) / 2)\n",
    "        b = (1 - 1 / num_tuples) * log(1 + 2 / (num_tuples - 1))\n",
    "        return a + b\n",
    "\n",
    "\n",
    "def calculate_delta(num_attributes, sensitivity, epsilon):\n",
    "# Computing delta, which is a factor when applying differential privacy.\n",
    "    return (num_attributes - 1) * sensitivity / epsilon\n",
    "\n",
    "\n",
    "def usefulness_minus_target(k, num_attributes, num_tuples, target_usefulness=5, epsilon=0.1):\n",
    " # Usefulness function used to calculate the value of K(degree of the bayesian network)\n",
    "\n",
    "    if k == num_attributes:\n",
    "        print('here')\n",
    "        usefulness = target_usefulness\n",
    "    else:\n",
    "        usefulness = num_tuples * epsilon / ((num_attributes - k) * (2 ** (k + 3)))  \n",
    "    return usefulness - target_usefulness\n",
    "\n",
    "\n",
    "def calculate_k(num_attributes, num_tuples, target_usefulness=4, epsilon=0.1):\n",
    "# Calculate the maximum degree when constructing Bayesian networks. \n",
    "    default_k = 3\n",
    "    initial_usefulness = usefulness_minus_target(default_k, num_attributes, num_tuples, 0, epsilon)\n",
    "    if initial_usefulness > target_usefulness:\n",
    "        return default_k\n",
    "    else:\n",
    "        arguments = (num_attributes, num_tuples, target_usefulness, epsilon)\n",
    "        warnings.filterwarnings(\"error\")\n",
    "        try:\n",
    "            ans = fsolve(usefulness_minus_target, np.array([int(num_attributes / 2)]), args=arguments)[0]\n",
    "            ans = ceil(ans)\n",
    "        except RuntimeWarning:\n",
    "            print(\"k is not properly computed!\")\n",
    "            ans = default_k\n",
    "        if ans < 1 or ans > num_attributes:\n",
    "            ans = default_k\n",
    "        return ans\n",
    "\n",
    "\n",
    "def set_random_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "def mutual_information(labels_x: Series, labels_y: DataFrame):\n",
    "# Mutual information of distributions in format of Series or DataFrame using inbuilt mutual_info_score lib fromsklearn.metrics\n",
    "#   Parameters\n",
    "# ----------\n",
    "# labels_x : Series\n",
    "#labels_y : DataFrame\n",
    "    if labels_y.shape[1] == 1:\n",
    "        labels_y = labels_y.iloc[:, 0]\n",
    "    else:\n",
    "        labels_y = labels_y.apply(lambda x: ' '.join(x.values), axis=1)\n",
    "\n",
    "    return mutual_info_score(labels_x, labels_y)\n",
    "def normalize_given_distribution(frequencies):\n",
    "    distribution = np.array(frequencies, dtype=float)\n",
    "    distribution = distribution.clip(0)  # replace negative values with 0\n",
    "    summation = distribution.sum()\n",
    "    if summation > 0:\n",
    "        if np.isinf(summation):\n",
    "            return normalize_given_distribution(np.isinf(distribution))\n",
    "        else:\n",
    "            return distribution / summation\n",
    "    else:\n",
    "        return np.full_like(distribution, 1 / distribution.size)\n",
    "\n",
    "\n",
    "\n",
    "def worker(paras):\n",
    "    child, V, num_parents, split, dataset = paras\n",
    "    parents_pair_list = []\n",
    "    mutual_info_list = []\n",
    "\n",
    "    if split + num_parents - 1 < len(V):\n",
    "        for other_parents in combinations(V[split + 1:], num_parents - 1):\n",
    "            parents = list(other_parents)\n",
    "            parents.append(V[split])\n",
    "            parents_pair_list.append((child, parents))\n",
    "            # TODO consider to change the computation of MI by combined integers instead of strings.\n",
    "            mi = mutual_information(dataset[child], dataset[parents])\n",
    "            mutual_info_list.append(mi)\n",
    "\n",
    "    return parents_pair_list, mutual_info_list\n",
    "\n",
    "\n",
    "def greedy_bayes(dataset: DataFrame, k: int, epsilon: float, seed=0):\n",
    "#Construct a Bayesian Network (BN) using greedy algorithm.\n",
    "# dataset : DataFrame\n",
    "#INPUT DATASET SHOULD ONLY CONTAIN CATEGORICAL ATTRIBUTES.\n",
    "# k : int\n",
    "#Maximum degree of the constructed BN. If k=0, k is automatically calculated.\n",
    "#epsilon : float\n",
    "#Parameter of differential privacy.\n",
    "#seed : int or float\n",
    "#Seed for the randomness in BN generation.\n",
    "    set_random_seed(seed)\n",
    "    dataset: DataFrame = dataset.astype(str, copy=False)\n",
    "    num_tuples, num_attributes = dataset.shape\n",
    "    if not k:\n",
    "        k = calculate_k(num_attributes, num_tuples)\n",
    "\n",
    "    attr_to_is_binary = {attr: dataset[attr].unique().size <= 2 for attr in dataset}\n",
    "    #a dictionary with key values as names of the attributes and the value indicates weather it is binary or not (no of values it has is 2 or not)\n",
    "\n",
    "    print('================ Constructing Bayesian Network (BN) ================')\n",
    "    root_attribute = random.choice(dataset.columns)\n",
    "    V = [root_attribute]\n",
    "    rest_attributes = list(dataset.columns)\n",
    "    rest_attributes.remove(root_attribute)\n",
    "    print(f'Adding ROOT {root_attribute}')\n",
    "    N = []\n",
    "    while rest_attributes:\n",
    "        parents_pair_list = []\n",
    "        mutual_info_list = []\n",
    "\n",
    "        num_parents = min(len(V), k)\n",
    "        tasks = [(child, V, num_parents, split, dataset) for child, split in\n",
    "                 product(rest_attributes, range(len(V) - num_parents + 1))]\n",
    "        with Pool() as pool:\n",
    "            res_list = pool.map(worker, tasks)\n",
    "\n",
    "        for res in res_list:\n",
    "            parents_pair_list += res[0]\n",
    "            mutual_info_list += res[1]\n",
    "\n",
    "        if epsilon:\n",
    "            sampling_distribution = exponential_mechanism(epsilon, mutual_info_list, parents_pair_list, attr_to_is_binary,\n",
    "                                                          num_tuples, num_attributes)\n",
    "            idx = np.random.choice(list(range(len(mutual_info_list))), p=sampling_distribution)\n",
    "        else:\n",
    "            idx = mutual_info_list.index(max(mutual_info_list))\n",
    "\n",
    "        N.append(parents_pair_list[idx])\n",
    "        adding_attribute = parents_pair_list[idx][0]\n",
    "        V.append(adding_attribute)\n",
    "        rest_attributes.remove(adding_attribute)\n",
    "        print(f'Adding attribute {adding_attribute}')\n",
    "\n",
    "    print('========================== BN constructed ==========================')\n",
    "\n",
    "    return N\n",
    "def display_bayesian_network(bn):\n",
    "#to display bayesian network\n",
    "    length = 0\n",
    "    for child, _ in bn:\n",
    "        if len(child) > length:\n",
    "            length = len(child)\n",
    "\n",
    "    print('Constructed Bayesian network:')\n",
    "    for child, parents in bn:\n",
    "        print(\"    {0:{width}} has parents {1}.\".format(child, parents, width=length))\n",
    "\n",
    "\n",
    "def exponential_mechanism(epsilon, mutual_info_list, parents_pair_list, attr_to_is_binary, num_tuples, num_attributes):\n",
    "#  Applied in Exponential Mechanism to sample outcomes\n",
    "    delta_array = []\n",
    "    for (child, parents) in parents_pair_list:\n",
    "        sensitivity = calculate_sensitivity(num_tuples, child, parents, attr_to_is_binary)\n",
    "        delta = calculate_delta(num_attributes, sensitivity, epsilon)\n",
    "        delta_array.append(delta)\n",
    "\n",
    "    mi_array = np.array(mutual_info_list) / (2 * np.array(delta_array))\n",
    "    mi_array = np.exp(mi_array)\n",
    "    mi_array = normalize_given_distribution(mi_array)\n",
    "    return mi_array\n",
    "\n",
    "\n",
    "def laplace_noise_parameter(k, num_attributes, num_tuples, epsilon):\n",
    "# The noises injected into conditional distributions.\n",
    "#such that it satisfies the differential privacy.\n",
    "# Note that these noises are over counts, instead of the probability distributions \n",
    "    return (num_attributes - k) / epsilon\n",
    "\n",
    "\n",
    "def get_noisy_distribution_of_attributes(attributes, encoded_dataset, epsilon=0.1):\n",
    "#it is to add noise to the attribute values \n",
    "#the encoded_dataset is the converted set of data(the categorical data is changed to numerical data)\n",
    "    data = encoded_dataset.copy().loc[:, attributes]\n",
    "    data['count'] = 1\n",
    "    stats = data.groupby(attributes).sum()\n",
    "\n",
    "    iterables = [range(int(encoded_dataset[attr].max()) + 1) for attr in attributes]\n",
    "    products = product(*iterables)\n",
    "\n",
    "    def grouper_it(iterable, n):\n",
    "        while True:\n",
    "            chunk_it = islice(iterable, n)\n",
    "            try:\n",
    "                first_el = next(chunk_it)\n",
    "            except StopIteration:\n",
    "                return\n",
    "            yield chain((first_el,), chunk_it)\n",
    "\n",
    "    full_space = None\n",
    "    for item in grouper_it(products, 1000000):\n",
    "        if full_space is None:\n",
    "            full_space = DataFrame(columns=attributes, data=list(item))\n",
    "        else:\n",
    "            data_frame_append = DataFrame(columns=attributes, data=list(item))\n",
    "            full_space = pd.concat([full_space, data_frame_append], ignore_index=True)\n",
    "\n",
    "    stats.reset_index(inplace=True)\n",
    "    stats = merge(full_space, stats, how='left')\n",
    "    stats.fillna(0, inplace=True)\n",
    "\n",
    "    if epsilon:\n",
    "        k = len(attributes) - 1\n",
    "        num_tuples, num_attributes = encoded_dataset.shape\n",
    "        noise_para = laplace_noise_parameter(k, num_attributes, num_tuples, epsilon)\n",
    "        laplace_noises = np.random.laplace(0, scale=noise_para, size=stats.index.size)\n",
    "        stats['count'] += laplace_noises\n",
    "        stats.loc[stats['count'] < 0, 'count'] = 0\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def construct_noisy_conditional_distributions(bayesian_network, encoded_dataset, epsilon=0.1):\n",
    "#to add noise to the conditional distributions \n",
    "    k = len(bayesian_network[-1][1])\n",
    "    conditional_distributions = {}\n",
    "\n",
    "    # first k+1 attributes\n",
    "    root = bayesian_network[0][1][0]\n",
    "    kplus1_attributes = [root]\n",
    "    for child, _ in bayesian_network[:k]:\n",
    "        kplus1_attributes.append(child)\n",
    "\n",
    "    noisy_dist_of_kplus1_attributes = get_noisy_distribution_of_attributes(kplus1_attributes, encoded_dataset, epsilon)\n",
    "\n",
    "    # generate noisy distribution of root attribute.\n",
    "    root_stats = noisy_dist_of_kplus1_attributes.loc[:, [root, 'count']].groupby(root).sum()['count']\n",
    "    conditional_distributions[root] = normalize_given_distribution(root_stats).tolist()\n",
    "\n",
    "    for idx, (child, parents) in enumerate(bayesian_network):\n",
    "        conditional_distributions[child] = {}\n",
    "\n",
    "        if idx <= k - 2:\n",
    "            stats = noisy_dist_of_kplus1_attributes.copy().loc[:, parents + [child, 'count']]\n",
    "            stats = stats.groupby(parents + [child], as_index=False).sum()\n",
    "        elif idx == k - 1:\n",
    "            stats = noisy_dist_of_kplus1_attributes.loc[:, parents + [child, 'count']]\n",
    "        else:\n",
    "            stats = get_noisy_distribution_of_attributes(parents + [child], encoded_dataset, epsilon)\n",
    "            stats = stats.loc[:, parents + [child, 'count']]\n",
    "\n",
    "        parents_grouper = parents[0] if len(parents) == 1 else parents\n",
    "        for parents_instance, stats_sub in stats.groupby(parents_grouper):\n",
    "            stats_sub = stats_sub.sort_values(by=child)\n",
    "            dist = normalize_given_distribution(stats_sub['count']).tolist()\n",
    "\n",
    "            parents_key = str([parents_instance]) if len(parents) == 1 else str(list(parents_instance))\n",
    "            conditional_distributions[child][parents_key] = dist\n",
    "\n",
    "    return conditional_distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e752c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDescriber:\n",
    "#Model input dataset, then save a description of the dataset into a json file\n",
    "#Number of bins in histograms.\n",
    " #       If it is a string such as 'auto' or 'fd', calculate the optimal bin width by `numpy.histogram_bin_edges`.\n",
    "  #  category_threshold : int\n",
    "   #     Categorical variables have no more than \"this number\" of distinct values.\n",
    "    #null_values: str or list\n",
    "   #     Additional strings to recognize as missing values.\n",
    "#    By default missing values already include {‘’, ‘NULL’, ‘N/A’, ‘NA’, ‘NaN’, ‘nan’}.\n",
    " #   attr_to_datatype : dict\n",
    " #       Dictionary of {attribute: datatype}, e.g., {\"age\": \"Integer\", \"gender\": \"String\"}.\n",
    " #   attr_to_is_categorical : dict\n",
    " #       Dictionary of {attribute: boolean}, e.g., {\"gender\":True, \"age\":False}.\n",
    " #   attr_to_is_candidate_key: dict\n",
    " #       Dictionary of {attribute: boolean}, e.g., {\"id\":True, \"name\":False}.\n",
    " #   data_description: dict\n",
    " #       Nested dictionary (equivalent to JSON) recording the mined dataset information.\n",
    " #   df_input : DataFrame\n",
    " #       The input dataset to be analyzed.\n",
    " #   attr_to_column : Dict\n",
    " #       Dictionary of {attribute: AbstractAttribute}\n",
    " #   bayesian_network : list\n",
    " #       List of [child, [parent,]] to represent a Bayesian Network.\n",
    " #   df_encoded : DataFrame\n",
    " #       Input dataset encoded into integers, taken as input by PrivBayes algorithm in correlated attribute mode.\n",
    "\n",
    "    def __init__(self, histogram_bins: Union[int, str] = 20, category_threshold=20, null_values=None):\n",
    "        self.histogram_bins: Union[int, str] = histogram_bins\n",
    "        self.category_threshold: int = category_threshold\n",
    "        self.null_values = null_values\n",
    "\n",
    "        self.attr_to_datatype: Dict[str, DataType] = None\n",
    "        self.attr_to_is_categorical: Dict[str, bool] = None\n",
    "        self.attr_to_is_candidate_key: Dict[str, bool] = None\n",
    "\n",
    "        self.data_description: Dict = {}\n",
    "        self.df_input: DataFrame = None\n",
    "        self.attr_to_column: Dict[str, AbstractAttribute] = None\n",
    "        self.bayesian_network: List = None\n",
    "        self.df_encoded: DataFrame = None\n",
    "\n",
    "    def describe_dataset_in_random_mode(self,\n",
    "                                        dataset_file: str,\n",
    "                                        attribute_to_datatype: Dict[str, DataType] = None,\n",
    "                                        attribute_to_is_categorical: Dict[str, bool] = None,\n",
    "                                        attribute_to_is_candidate_key: Dict[str, bool] = None,\n",
    "                                        categorical_attribute_domain_file: str = None,\n",
    "                                        numerical_attribute_ranges: Dict[str, List] = None,\n",
    "                                        seed=0):\n",
    "        attribute_to_datatype = attribute_to_datatype or {}\n",
    "        attribute_to_is_categorical = attribute_to_is_categorical or {}\n",
    "        attribute_to_is_candidate_key = attribute_to_is_candidate_key or {}\n",
    "        numerical_attribute_ranges = numerical_attribute_ranges or {}\n",
    "\n",
    "        if categorical_attribute_domain_file:\n",
    "            categorical_attribute_to_domain = utils.read_json_file(categorical_attribute_domain_file)\n",
    "        else:\n",
    "            categorical_attribute_to_domain = {}\n",
    "\n",
    "        utils.set_random_seed(seed)\n",
    "        self.attr_to_datatype = {attr: DataType(datatype) for attr, datatype in attribute_to_datatype.items()}\n",
    "        self.attr_to_is_categorical = attribute_to_is_categorical\n",
    "        self.attr_to_is_candidate_key = attribute_to_is_candidate_key\n",
    "        self.read_dataset_from_csv(dataset_file)\n",
    "        self.infer_attribute_data_types()\n",
    "        self.analyze_dataset_meta()\n",
    "        self.represent_input_dataset_by_columns()\n",
    "\n",
    "        for column in self.attr_to_column.values():\n",
    "            attr_name = column.name\n",
    "            if attr_name in categorical_attribute_to_domain:\n",
    "                column.infer_domain(categorical_domain=categorical_attribute_to_domain[attr_name])\n",
    "            elif attr_name in numerical_attribute_ranges:\n",
    "                column.infer_domain(numerical_range=numerical_attribute_ranges[attr_name])\n",
    "            else:\n",
    "                column.infer_domain()\n",
    "\n",
    "        # record attribute information in json format\n",
    "        self.data_description['attribute_description'] = {}\n",
    "        for attr, column in self.attr_to_column.items():\n",
    "            self.data_description['attribute_description'][attr] = column.to_json()\n",
    "\n",
    "    def describe_dataset_in_independent_attribute_mode(self,\n",
    "                                                       dataset_file,\n",
    "                                                       epsilon=0.1,\n",
    "                                                       attribute_to_datatype: Dict[str, DataType] = None,\n",
    "                                                       attribute_to_is_categorical: Dict[str, bool] = None,\n",
    "                                                       attribute_to_is_candidate_key: Dict[str, bool] = None,\n",
    "                                                       categorical_attribute_domain_file: str = None,\n",
    "                                                       numerical_attribute_ranges: Dict[str, List] = None,\n",
    "                                                       seed=0):\n",
    "        self.describe_dataset_in_random_mode(dataset_file,\n",
    "                                             attribute_to_datatype,\n",
    "                                             attribute_to_is_categorical,\n",
    "                                             attribute_to_is_candidate_key,\n",
    "                                             categorical_attribute_domain_file,\n",
    "                                             numerical_attribute_ranges,\n",
    "                                             seed=seed)\n",
    "\n",
    "        for column in self.attr_to_column.values():\n",
    "            column.infer_distribution()\n",
    "\n",
    "        self.inject_laplace_noise_into_distribution_per_attribute(epsilon)\n",
    "        # record attribute information in json format\n",
    "        self.data_description['attribute_description'] = {}\n",
    "        for attr, column in self.attr_to_column.items():\n",
    "            self.data_description['attribute_description'][attr] = column.to_json()\n",
    "\n",
    "    def describe_dataset_in_correlated_attribute_mode(self,\n",
    "                                                      dataset_file,\n",
    "                                                      k=0,\n",
    "                                                      epsilon=0.1,\n",
    "                                                      attribute_to_datatype: Dict[str, DataType] = None,\n",
    "                                                      attribute_to_is_categorical: Dict[str, bool] = None,\n",
    "                                                      attribute_to_is_candidate_key: Dict[str, bool] = None,\n",
    "                                                      categorical_attribute_domain_file: str = None,\n",
    "                                                      numerical_attribute_ranges: Dict[str, List] = None,\n",
    "                                                      seed=0):\n",
    "  \n",
    "        \n",
    "        self.describe_dataset_in_independent_attribute_mode(dataset_file,\n",
    "                                                            epsilon,\n",
    "                                                            attribute_to_datatype,\n",
    "                                                            attribute_to_is_categorical,\n",
    "                                                            attribute_to_is_candidate_key,\n",
    "                                                            categorical_attribute_domain_file,\n",
    "                                                            numerical_attribute_ranges,\n",
    "                                                            seed)\n",
    "        self.df_encoded = self.encode_dataset_into_binning_indices()\n",
    "        if self.df_encoded.shape[1] < 2:\n",
    "            raise Exception(\"Correlated Attribute Mode requires at least 2 attributes(i.e., columns) in dataset.\")\n",
    "\n",
    "        self.bayesian_network = greedy_bayes(self.df_encoded, k, epsilon / 2, seed=seed)\n",
    "        self.data_description['bayesian_network'] = self.bayesian_network\n",
    "        self.data_description['conditional_probabilities'] = construct_noisy_conditional_distributions(\n",
    "            self.bayesian_network, self.df_encoded, epsilon / 2)\n",
    "\n",
    "    def read_dataset_from_csv(self, file_name=None):\n",
    "        try:\n",
    "            self.df_input = read_csv(file_name, skipinitialspace=True, na_values=self.null_values)\n",
    "        except (UnicodeDecodeError, NameError):\n",
    "            self.df_input = read_csv(file_name, skipinitialspace=True, na_values=self.null_values,\n",
    "                                     encoding='latin1')\n",
    "\n",
    "        # Remove columns with empty active domain, i.e., all values are missing.\n",
    "        attributes_before = set(self.df_input.columns)\n",
    "        self.df_input.dropna(axis=1, how='all')\n",
    "        attributes_after = set(self.df_input.columns)\n",
    "        if len(attributes_before) > len(attributes_after):\n",
    "            print(f'Empty columns are removed, including {attributes_before - attributes_after}.')\n",
    "\n",
    "    def infer_attribute_data_types(self):\n",
    "        attributes_with_unknown_datatype = set(self.df_input.columns) - set(self.attr_to_datatype)\n",
    "        inferred_numerical_attributes = utils.infer_numerical_attributes_in_dataframe(self.df_input)\n",
    "\n",
    "        for attr in attributes_with_unknown_datatype:\n",
    "            column_dropna = self.df_input[attr].dropna()\n",
    "\n",
    "            # current attribute is either Integer or Float.\n",
    "            if attr in inferred_numerical_attributes:\n",
    "                # TODO Comparing all values may be too slow for large datasets.\n",
    "                if array_equal(column_dropna, column_dropna.astype(int, copy=False)):\n",
    "                    self.attr_to_datatype[attr] = DataType.INTEGER\n",
    "                else:\n",
    "                    self.attr_to_datatype[attr] = DataType.FLOAT\n",
    "\n",
    "            # current attribute is either String, DateTime, or SocialSecurityNumber.\n",
    "            else:\n",
    "                # Sample 20 values to test its data_type.\n",
    "                samples = column_dropna.sample(20, replace=True)\n",
    "                if all(samples.map(is_datetime)):\n",
    "                    self.attr_to_datatype[attr] = DataType.DATETIME\n",
    "                else:\n",
    "                    if all(samples.map(is_ssn)):\n",
    "                        self.attr_to_datatype[attr] = DataType.SOCIAL_SECURITY_NUMBER\n",
    "                    else:\n",
    "                        self.attr_to_datatype[attr] = DataType.STRING\n",
    "\n",
    "    def analyze_dataset_meta(self):\n",
    "        all_attributes = set(self.df_input.columns)\n",
    "\n",
    "        # find all candidate keys.\n",
    "        for attr in all_attributes - set(self.attr_to_is_candidate_key):\n",
    "            if self.attr_to_datatype[attr] in {DataType.FLOAT, DataType.DATETIME}:\n",
    "                self.attr_to_is_candidate_key[attr] = False\n",
    "            else:\n",
    "                self.attr_to_is_candidate_key[attr] = self.df_input[attr].is_unique\n",
    "\n",
    "        candidate_keys = {attr for attr, is_key in self.attr_to_is_candidate_key.items() if is_key}\n",
    "\n",
    "        # find all categorical attributes.\n",
    "        for attr in all_attributes - set(self.attr_to_is_categorical):\n",
    "            self.attr_to_is_categorical[attr] = self.is_categorical(attr)\n",
    "\n",
    "        non_categorical_string_attributes = set()\n",
    "        for attr, is_categorical in self.attr_to_is_categorical.items():\n",
    "            if not is_categorical and self.attr_to_datatype[attr] is DataType.STRING:\n",
    "                non_categorical_string_attributes.add(attr)\n",
    "\n",
    "        attributes_in_BN = [attr for attr in self.df_input if\n",
    "                            attr not in candidate_keys and attr not in non_categorical_string_attributes]\n",
    "        non_categorical_string_attributes = list(non_categorical_string_attributes)\n",
    "\n",
    "        self.data_description['meta'] = {\"num_tuples\": self.df_input.shape[0],\n",
    "                                         \"num_attributes\": self.df_input.shape[1],\n",
    "                                         \"num_attributes_in_BN\": len(attributes_in_BN),\n",
    "                                         \"all_attributes\": self.df_input.columns.tolist(),\n",
    "                                         \"candidate_keys\": list(candidate_keys),\n",
    "                                         \"non_categorical_string_attributes\": non_categorical_string_attributes,\n",
    "                                         \"attributes_in_BN\": attributes_in_BN}\n",
    "\n",
    "    def is_categorical(self, attribute_name):\n",
    "#Detect whether an attribute is categorical\n",
    "\n",
    "        if attribute_name in self.attr_to_is_categorical:\n",
    "            return self.attr_to_is_categorical[attribute_name]\n",
    "        else:\n",
    "            return self.df_input[attribute_name].dropna().unique().size <= self.category_threshold\n",
    "\n",
    "    def represent_input_dataset_by_columns(self):\n",
    "        self.attr_to_column = {}\n",
    "        for attr in self.df_input:\n",
    "            data_type = self.attr_to_datatype[attr]\n",
    "            is_candidate_key = self.attr_to_is_candidate_key[attr]\n",
    "            is_categorical = self.attr_to_is_categorical[attr]\n",
    "            paras = (attr, is_candidate_key, is_categorical, self.histogram_bins, self.df_input[attr])\n",
    "            if data_type is DataType.INTEGER:\n",
    "                self.attr_to_column[attr] = IntegerAttribute(*paras)\n",
    "            elif data_type is DataType.FLOAT:\n",
    "                self.attr_to_column[attr] = FloatAttribute(*paras)\n",
    "            elif data_type is DataType.DATETIME:\n",
    "                self.attr_to_column[attr] = DateTimeAttribute(*paras)\n",
    "            elif data_type is DataType.STRING:\n",
    "                self.attr_to_column[attr] = StringAttribute(*paras)\n",
    "            elif data_type is DataType.SOCIAL_SECURITY_NUMBER:\n",
    "                self.attr_to_column[attr] = SocialSecurityNumberAttribute(*paras)\n",
    "            else:\n",
    "                raise Exception(f'The DataType of {attr} is unknown.')\n",
    "\n",
    "    def inject_laplace_noise_into_distribution_per_attribute(self, epsilon=0.1):\n",
    "        num_attributes_in_BN = self.data_description['meta']['num_attributes_in_BN']\n",
    "        for column in self.attr_to_column.values():\n",
    "            assert isinstance(column, AbstractAttribute)\n",
    "            column.inject_laplace_noise(epsilon, num_attributes_in_BN)\n",
    "\n",
    "    def encode_dataset_into_binning_indices(self):\n",
    " #       \"\"\"Before constructing Bayesian network, encode input dataset into binning indices.\"\"\"\n",
    "        encoded_dataset = DataFrame()\n",
    "        for attr in self.data_description['meta']['attributes_in_BN']:\n",
    "            encoded_dataset[attr] = self.attr_to_column[attr].encode_values_into_bin_idx()\n",
    "        return encoded_dataset\n",
    "\n",
    "    def save_dataset_description_to_file(self, file_name):\n",
    "        Path(file_name).touch()\n",
    "        with open(file_name, 'w') as outfile:\n",
    "            json.dump(self.data_description, outfile, indent=4)\n",
    "\n",
    "    def display_dataset_description(self):\n",
    "        print(json.dumps(self.data_description, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb5033",
   "metadata": {},
   "source": [
    "This function takes the input data and preprocessing takes place here using the functoins defined above and it gets the info in the abstract attribute format and saves the description file in json form. \n",
    "This file in json form should used in the datagenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.synthetic_dataset = None\n",
    "        self.description = {}\n",
    "        self.encoded_dataset = None\n",
    "\n",
    "    def generate_dataset_in_random_mode(self, n, description_file, seed=0, minimum=0, maximum=100):\n",
    "        set_random_seed(seed)\n",
    "        description = read_json_file(description_file)\n",
    "\n",
    "        self.synthetic_dataset = DataFrame()\n",
    "        for attr in description['attribute_description'].keys():\n",
    "            attr_info = description['attribute_description'][attr]\n",
    "            datatype = attr_info['data_type']\n",
    "            is_categorical = attr_info['is_categorical']\n",
    "            is_candidate_key = attr_info['is_candidate_key']\n",
    "            if is_candidate_key:\n",
    "                self.synthetic_dataset[attr] = parse_json(attr_info).generate_values_as_candidate_key(n)\n",
    "            elif is_categorical:\n",
    "                self.synthetic_dataset[attr] = random.choice(attr_info['distribution_bins'], n)\n",
    "            elif datatype == 'String':\n",
    "                length = random.randint(attr_info['min'], attr_info['max'] + 1)\n",
    "                self.synthetic_dataset[attr] = length\n",
    "                self.synthetic_dataset[attr] = self.synthetic_dataset[attr].map(lambda x: generate_random_string(x))\n",
    "            else:\n",
    "                if datatype == 'Integer':\n",
    "                    self.synthetic_dataset[attr] = random.randint(minimum, maximum + 1, n)\n",
    "                else:\n",
    "                    self.synthetic_dataset[attr] = random.uniform(minimum, maximum, n)\n",
    "\n",
    "    def generate_dataset_in_independent_mode(self, n, description_file, seed=0):\n",
    "        set_random_seed(seed)\n",
    "        self.description = read_json_file(description_file)\n",
    "\n",
    "        all_attributes = self.description['meta']['all_attributes']\n",
    "        candidate_keys = set(self.description['meta']['candidate_keys'])\n",
    "        self.synthetic_dataset = DataFrame(columns=all_attributes)\n",
    "        for attr in all_attributes:\n",
    "            attr_info = self.description['attribute_description'][attr]\n",
    "            column = parse_json(attr_info)\n",
    "\n",
    "            if attr in candidate_keys:\n",
    "                self.synthetic_dataset[attr] = column.generate_values_as_candidate_key(n)\n",
    "            else:\n",
    "                binning_indices = column.sample_binning_indices_in_independent_attribute_mode(n)\n",
    "                self.synthetic_dataset[attr] = column.sample_values_from_binning_indices(binning_indices)\n",
    "\n",
    "    def generate_dataset_in_correlated_attribute_mode(self, n, description_file, seed=0):\n",
    "        set_random_seed(seed)\n",
    "        self.n = n\n",
    "        self.description = read_json_file(description_file)\n",
    "\n",
    "        all_attributes = self.description['meta']['all_attributes']\n",
    "        candidate_keys = set(self.description['meta']['candidate_keys'])\n",
    "        self.encoded_dataset = DataGenerator.generate_encoded_dataset(self.n, self.description)\n",
    "        self.synthetic_dataset = DataFrame(columns=all_attributes)\n",
    "        for attr in all_attributes:\n",
    "            attr_info = self.description['attribute_description'][attr]\n",
    "            column = parse_json(attr_info)\n",
    "\n",
    "            if attr in self.encoded_dataset:\n",
    "                self.synthetic_dataset[attr] = column.sample_values_from_binning_indices(self.encoded_dataset[attr])\n",
    "            elif attr in candidate_keys:\n",
    "                self.synthetic_dataset[attr] = column.generate_values_as_candidate_key(n)\n",
    "            else:\n",
    "                # for attributes not in BN or candidate keys, use independent attribute mode.\n",
    "                binning_indices = column.sample_binning_indices_in_independent_attribute_mode(n)\n",
    "                self.synthetic_dataset[attr] = column.sample_values_from_binning_indices(binning_indices)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sampling_order(bn):\n",
    "        order = [bn[0][1][0]]\n",
    "        for child, _ in bn:\n",
    "            order.append(child)\n",
    "        return order\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_encoded_dataset(n, description):\n",
    "        bn = description['bayesian_network']\n",
    "        bn_root_attr = bn[0][1][0]\n",
    "        root_attr_dist = description['conditional_probabilities'][bn_root_attr]\n",
    "        encoded_df = DataFrame(columns=DataGenerator.get_sampling_order(bn))\n",
    "        encoded_df[bn_root_attr] = random.choice(len(root_attr_dist), size=n, p=root_attr_dist)\n",
    "\n",
    "        for child, parents in bn:\n",
    "            child_conditional_distributions = description['conditional_probabilities'][child]\n",
    "            for parents_instance in child_conditional_distributions.keys():\n",
    "                dist = child_conditional_distributions[parents_instance]\n",
    "                parents_instance = list(eval(parents_instance))\n",
    "\n",
    "                filter_condition = ''\n",
    "                for parent, value in zip(parents, parents_instance):\n",
    "                    filter_condition += f\"(encoded_df['{parent}']=={value})&\"\n",
    "\n",
    "                filter_condition = eval(filter_condition[:-1])\n",
    "\n",
    "                size = encoded_df[filter_condition].shape[0]\n",
    "                if size:\n",
    "                    encoded_df.loc[filter_condition, child] = random.choice(len(dist), size=size, p=dist)\n",
    "\n",
    "            unconditioned_distribution = description['attribute_description'][child]['distribution_probabilities']\n",
    "            encoded_df.loc[encoded_df[child].isnull(), child] = random.choice(len(unconditioned_distribution),\n",
    "                                                                              size=encoded_df[child].isnull().sum(),\n",
    "                                                                              p=unconditioned_distribution)\n",
    "        encoded_df[encoded_df.columns] = encoded_df[encoded_df.columns].astype(int)\n",
    "        return encoded_df\n",
    "\n",
    "    def save_synthetic_data(self, to_file):\n",
    "        Path(to_file).touch()\n",
    "        self.synthetic_dataset.to_csv(to_file, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from time import time\n",
    "\n",
    "    dataset_description_file = '../out/AdultIncome/description_test.txt'\n",
    "    generator = DataGenerator()\n",
    "\n",
    "    t = time()\n",
    "    generator.generate_dataset_in_correlated_attribute_mode(51, dataset_description_file)\n",
    "    print('running time: {} s'.format(time() - t))\n",
    "    print(generator.synthetic_dataset.loc[:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
